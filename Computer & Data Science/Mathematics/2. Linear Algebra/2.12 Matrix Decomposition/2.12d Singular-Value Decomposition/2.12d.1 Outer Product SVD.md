>[!note] Outer Product Form of the SVD
>A matrix $A \in \mathbb{R}^{m \times n}$ with rank $r \leq \min\{m,n\}$ can be decomposed using the outer product form of the [[2.12d Singular-Value Decomposition|SVD]] as
>$$A = \sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T$$
>where $\{\vec{u}_1, \vec{u}_2, \dots, \vec{u}_r\} \subseteq \mathbb{R}^m$ and $\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_r\} \subseteq \mathbb{R}^n$ are sets of orthonormal vectors known respectively as the ***left singular vectors*** and ***right singular vectors*** of $A$, whereas $\sigma_1 > \dots > \sigma_r > 0$ are positive ordered scalars known as the ***singular values*** of $A$.

# Intuition
Essentially, we can algorithmically compute the SVD as follows:
1. Find and sort $(\lambda_1, \vec{v}_1),...,(\lambda_r, \vec{v}_r)$, the orthogonal eigenvalue-eigenvector pairs of $A^TA$.
2. For each pair, find:
	1. $\sigma_i=\sqrt{\lambda_i}$
	2. $\vec{u}_i=\frac{1}{\sigma_i}A\vec{v}_i$
We can prove the second step using $\frac{1}{\sigma_k}A\vec{v}_k=\frac{1}{\sigma_k}\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T\vec{v}_k$. Then $\vec{v}_i^T\vec{v}_k=\left<\vec{v}_k, \vec{v}_i\right>$ will be 0 for $i \neq k$ and 1 for $i = k$ since the right singular vectors are orthonormal. As such, $\frac{1}{\sigma_k}\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T\vec{v}_k=\vec{u}_k$.

Alternatively, we can find and sort $(\lambda_1, \vec{u}_1),...,(\lambda_r, \vec{u}_r)$, which are the eigenvalue-eigenvector pairs of $AA^T$. Then, $\vec{v}_i=\frac{1}{\sigma_i}A^T\vec{u}_i$ or $\vec{v}_i^T=\frac{1}{\upsigma_i}A\vec{u}_i^T$.

This SVD form is messy, albeit the most efficient to compute since it decomposes $A$ into $r$ rank-1 matrices.

# Properties
##### $\{\vec{u}_1, \dots, \vec{u}_r\}$ is an orthonormal basis of $\mathrm{Col}(A)$
Since $\{\vec{u}_1, \dots, \vec{u}_r\}$, we only need to prove that it spans $\mathrm{Col}(A)$:
$$
\begin{align}
A\vec{x} &= \left(\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T\right)\vec{x} \\
&=
\sum^{r}_{i=1}\sigma_i\vec{u}_i\left(\vec{v}_i^T\vec{x}\right) \\
&=
\sum^{r}_{i=1}\sigma_i\left<\vec{x},\vec{v}_i\right>\vec{u}_i.
\end{align}
$$
The result is a linear combination of $\vec{u}_i$. Therefore, $\mathrm{Col}(A) \subseteq \mathrm{Span}(\vec{u}_1,\dots,\vec{u}_r)$. Since both $\mathrm{Col}(A)$ and $\mathrm{Span}(\vec{u}_1,\dots,\vec{u}_r)$ have the same dimension $r$, they are equal.

#### $\{\vec{v}_1, \dots, \vec{v}_r\}$ is an orthonormal basis of $\mathrm{Row}(A)=\mathrm{Col}(A^T)$
Since $A^T=\sum^{r}_{i=1}\sigma_i\vec{v}_i\vec{u}_i^T$, we can proceed using the same reasoning as above.



#### $(\sigma_i^2,\vec{u}_i)$ is an eigenvalue-eigenvector pair of $AA^T$
For any $\vec{u}_k \in \{\vec{u}_1, \dots, \vec{u}_r\}$:
$$
\begin{align}
AA^T\vec{u}_k
&=
\left(\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T\right)\left(\sum^{r}_{j=1}\sigma_j\vec{v}_j\vec{u}_j^T\right)\vec{u}_k \\
&=
\sum^{r}_{i=1}\sum^{r}_{j=1}\sigma_i\sigma_j\vec{u}_i\vec{v}_i^T\vec{v}_j\vec{u}_j^T\vec{u}_k \\
&=
 \sum^{r}_{i=1}\sum^{r}_{j=1}\sigma_i\sigma_j\vec{u}_i\left<\vec{v}_j,\vec{v}_i\right>\left<\vec{u}_k,\vec{u}_j\right> \\
&=
\sigma^2_k\vec{u}_k.
\end{align}
$$
We obtain this from the fact that $\left<\vec{v}_j,\vec{v}_i\right>=\begin{cases}0 & \text{ } i \neq j \\ 1 & \text{   } i = j\end{cases}$  and $\left<\vec{u}_k,\vec{u}_j\right>=\begin{cases}0 & \text{ } k \neq j \\ 1 & \text{   } k = j\end{cases}$ since all elements of both sets are orthonormal to each other by set orthonormality. Therefore the equation is non-zero if and only if $k=i=j$.
#### $(\sigma_i^2,\vec{v}_i)$ is an eigenvalue-eigenvector pair of $A^TA$
Proceed by same reasoning as above.
#### $A\vec{v}_i=\sigma_i\vec{u}_i \iff A^T\vec{u}_i=\sigma_i\vec{v}_i$
$$
\begin{align}
A\vec{v}_k&=\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T\vec{v}_k \\
&= \sum^{r}_{i=1}\sigma_i\left<\vec{v}_k,\vec{v}_i\right>\vec{u}_i \\
&=\sigma_i\vec{u}
_i\end{align}
$$
Since due to orthonormality the equation is non-zero only when $k=i$.




# Application
TODO: Miki's Movies Data