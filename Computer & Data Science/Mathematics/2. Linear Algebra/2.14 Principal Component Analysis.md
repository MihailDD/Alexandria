>[!note] Principal Component Analysis
>For a data matrix $A=\begin{bmatrix}\vec{x}_1 & \dots & \vec{x}_n\end{bmatrix} \in \mathbb{R}^{d \times n}$ with SVD $A=U\Sigma V^T$,
>
>1. $$U_\mathscr{l} \in \sum^{n}_{i=1}\underset{W \in \mathbb{R}^{d \times \mathscr{l}}}{\mathrm{argmin}}||\vec{x}_i-W^TW\vec{x}_i||^2,$$
> where  $W \in \mathbb{R}^{d \times \mathscr{l}}$ is an [[2.9c Matrix Orthonormality|orthonormal matrix]] such that $W^TW=I_\mathscr{l}$, and $U_\mathscr{l}$ consists of the first $\mathscr{l}$ columns of the matrix of principal components $U$.
>
>2. $$S_{\text{PCA}} \in \sum^{n}_{i=1}\underset{S \subseteq \mathbb{R}^d}{\mathrm{argmin}}||\vec{x}_i-\mathrm{proj}_S(\vec{x}_i)\vec{x}_i||^2,$$
>where $\mathrm{dim}(S) \leq \mathscr{l}$, and $S_{\text{PCA}}=\mathrm{Span}(\vec{u}_1,\dots,\vec{u}_\mathscr{l})$ is the span of the first $\mathscr{l}$ columns of the matrix of principal components $U$.
>
>3. 
# Intuition
We can compute the PCA of a data matrix $A \in \mathbb{R}^{d \times n}$  by first centering $A$ around the mean of its datapoints ($\frac{1}{n}A\vec{1}_n$) s.t. $D = A(I_n - \frac{1}{n}\vec{1}_n\vec{1}_n^T)$. Then, $\mathrm{rank}(D)=r \leq \mathrm{min}\{m, n\}$. Suppose that the *ground truth* data matrix (containing the noiseless data) in reality spans an $\mathscr{l}$-dimensional subspace $S_{gt} \subseteq \mathbb{R}^d$, such that $\mathscr{l} \leq r$. Then, the remaining $r-\mathscr{l}$ dimensions of $D$ are due to noise. The goal of PCA is to extract the "best" $\mathscr{l}$-dimensional subspace from the noisy data such that it minimizes the distance between the real subspace and our approximation. We can see the differences between this optimization approach and least squares optimization below:

We can s