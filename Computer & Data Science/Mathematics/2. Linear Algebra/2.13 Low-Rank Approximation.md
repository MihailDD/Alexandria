>[!note] Eckart-Young-Minsky Theorem
>A [[2.4a Matrix|matrix]] $A \in \mathbb{R}^{m \times n}$ with rank $r \leq \mathrm{min}\{m, n\}$ can be approximated by a matrix $A_\mathscr{l}$ with rank $\mathscr{l} \leq r$, such that
>$$A_\mathscr{l} \in \underset{B \in \mathbb{R}^{m \times n}}{\mathrm{argmin}}||A-B||_F^2.$$
>We may then approximate our matrix $A$ using [[2.12d Singular-Value Decomposition|SVD]] as
>$$A_\mathscr{l}=\sum^{\mathscr{l}}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T=U_\mathscr{l}\Sigma_\mathscr{l}V_\mathscr{l}^T.$$
# Intuition
Essentially, the above definition states that $A_\mathscr{l}$ is found inside the set of matrices $B$ that minimize the square of the [[2.4g.1 Frobenius Norm|Frobenius norm]] of $A-B$ (in other words $A \approx B$). This holds mostly if the data is real, and therefore has some linear, low-rank structure with some singular values being large, and some very small. Then, we can equivalently say that the first $\mathscr{l}$ ordered singular values are large, and the remaining $r-\mathscr{l}$ are small:
$$\sum^{r}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T=\sum^{\mathscr{l}}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T+\sum^{r}_{i=\mathscr{l}+1}\underbrace{\sigma_i}_{\approx 0}\vec{u}_i\vec{v}_i^T=\sum^{\mathscr{l}}_{i=1}\sigma_i\vec{u}_i\vec{v}_i^T.$$
